{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision.transforms import transforms\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class State:\n",
    "    frame: torch.Tensor\n",
    "    sp: float       # stamina points\n",
    "    zoom: int\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "    direction: int\n",
    "    time_steps: int\n",
    "\n",
    "# Check if frame has stone\n",
    "def check_frame(self, frame) -> bool:\n",
    "    crops = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            crop = torchvision.transforms.functional.crop(frame, i * 24, j * 24, 24, 24)[None]\n",
    "            crops.append(crop)\n",
    "    expected_stone = self.stone_classifier(torch.cat(crops)).max(dim=0)[0][1].item()\n",
    "    return expected_stone > 0.5\n",
    "\n",
    "def find_state_with_stone(df: pd.DataFrame, max_attempt: int = 10) -> State:\n",
    "    while True:\n",
    "        index = np.random.randint(0, len(df) - 1)\n",
    "        frame = load_image(df[\"video\"][index], df[\"frame\"][index])\n",
    "        train_aug = A.Compose([A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "                               ToTensorV2(transpose_mask=False),\n",
    "                               ])\n",
    "        frame = train_aug(image=frame)['image']\n",
    "        if env.check_frame(frame):\n",
    "            break\n",
    "    sp = df[\"sp\"][index]\n",
    "    zoom = df[\"zoom\"][index]\n",
    "    state = State(frame, sp, zoom)\n",
    "    return state\n",
    "\n",
    "def see_plot(pict, size=(6, 6), title: str = None):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(pict, cmap='gray')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def load_image(video, frame):\n",
    "    path = '../surviv_rl_data/all_videoframes_rgb_96/{}/'.format(video)\n",
    "    p = cv2.imread(path + 'f_{}.jpg'.format(frame))\n",
    "    return p[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet_v2(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3])  # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5])  # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        self.act_last = nn.Tanh()\n",
    "        self.support_conv1 = nn.Conv2d(11, 512, 1)  # (bath,10+1) --> (batch,512)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x_original = self.conv_original_size0(inp[0])\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(inp[0])\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        cond = self.support_conv1(torch.unsqueeze(torch.unsqueeze(inp[1], 2), 2))  # ([8, 8]) --> Size([8, 512, 1, 1])\n",
    "        layer4 = self.layer4_1x1(layer4 + cond)\n",
    "\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        out = self.conv_last(x)\n",
    "        out = self.act_last(out)\n",
    "\n",
    "        return out\n",
    "#====================================================================    \n",
    "    \n",
    "class StoneClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, 2, 1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, 2, 1)\n",
    "        self.fc1 = nn.Linear(32 * 3 * 3, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "class NeuralEnv:\n",
    "    def __init__(self,\n",
    "                 env_model_path: str,\n",
    "                 reward_model_path: str,\n",
    "                 device: str,\n",
    "                 batch_size = 16,\n",
    "                 reward_confidence=0.5,\n",
    "                 stone_frac=0.0,\n",
    "                 step_size=4,\n",
    "                 max_step=14):\n",
    "        '''\n",
    "        input params:\n",
    "            env_model_path    [str] : path to model s_next=model(s_curr,action)\n",
    "            reward_model_path [str] : path to model reward=model(s_curr)\n",
    "            device            [str] : one of {'cpu', 'cuda:0', 'cuda:1'}\n",
    "            batch_size        [int] : len of batch\n",
    "            reward_confidence [flt] : classificator's confidence\n",
    "            stone_frac        [flt] : part of the initial states with guaranteed stones\n",
    "            step_size         [int] : \n",
    "            max_step          [int] : \n",
    "        output params:\n",
    "            all output-variables will be torch.tensors in the selected DEVICE\n",
    "            all input-variables have to be torch.tensors in the selected DEVICE\n",
    "        '''\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.reward_confidence = reward_confidence\n",
    "        self.stone_frac = stone_frac\n",
    "        self.step_size = step_size\n",
    "        self.max_step = max_step\n",
    "        self.reward_frame_transform = transforms.Compose([transforms.CenterCrop(24)])\n",
    "        self.frame_transform = A.Compose([A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "                                          ToTensorV2(transpose_mask=False)])\n",
    "        \n",
    "        self.model = ResNetUNet_v2(3)\n",
    "        self.model.load_state_dict(torch.load(env_model_path, map_location=self.device))\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.stone_classifier = StoneClassifier()\n",
    "        self.stone_classifier.load_state_dict(torch.load(reward_model_path))\n",
    "        self.stone_classifier = self.stone_classifier.to(self.device)\n",
    "        self.stone_classifier.eval()\n",
    "\n",
    "        self.df = pd.read_csv('../surviv_rl_data/dataset_inventory_v2.csv')\n",
    "        self.df = self.df[self.df.zoom == 1].reset_index()\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    def reset(self):\n",
    "        '''\n",
    "        output params:\n",
    "            init_s     [float torch tensor [-1...1]] : batch of initial states (batch,3,96,96)\n",
    "            init_supp  [float torch tensor]          : batch of initial support vector (batch,2)\n",
    "        '''\n",
    "        init_s = torch.zeros(self.batch_size,3,96,96).float()\n",
    "        init_supp = torch.zeros(self.batch_size,2).float()\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            j = np.random.randint(len(self.df))\n",
    "            frame = load_image(self.df[\"video\"][j], self.df[\"frame\"][j])\n",
    "            frame = self.frame_transform(image=frame)['image']\n",
    "            supp = torch.tensor([self.df[\"sp\"][j]/100,self.df[\"zoom\"][j]/15]).float() \n",
    "            #if check_frame(frame)==True:\n",
    "            #    init_s[i] = frame\n",
    "            #    init_supp[i] = supp\n",
    "            init_s[i] = frame\n",
    "            init_supp[i] = supp\n",
    "        return init_s.to(self.device),init_supp.to(self.device)\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        '''\n",
    "        input params:\n",
    "            state [float torch.tensor [-1...1]] : batch of states (batch,3,96,96)\n",
    "        output params:\n",
    "            r      [float torch.tensor [0...1]]  : batch of rewards (batch,1)\n",
    "        '''\n",
    "        state = self.reward_frame_transform(state)\n",
    "        with torch.no_grad():\n",
    "            r = self.stone_classifier(state)[:,1].unsqueeze(1)\n",
    "        r = (r>self.reward_confidence).float().detach()\n",
    "        return r\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def step(self, s_curr, supp_curr, action):\n",
    "        '''\n",
    "        input params:\n",
    "            s_curr    [float torch.tensor [-1...1]] : batch of current states (batch,3,96,96)\n",
    "            supp_curr [float torch tensor]          : batch of current support vector (batch,2)\n",
    "            action    [int torch tensor {1,...,8}]  : batch of chosen direction (batch,1)  \n",
    "        output params:\n",
    "            s_next    [float torch.tensor [-1...1]] : batch of next states (batch,3,96,96)\n",
    "            supp_next [float torch tensor]          : batch of next support vector =supp_curr (batch,2)\n",
    "            reward    [float torch.tensor [0...1]]  : batch of rewards (batch,1)\n",
    "        '''\n",
    "        action_ohe = F.one_hot(action.squeeze()-1, num_classes=8).float() # (batch,8)\n",
    "        if len(action_ohe.shape) == 1:\n",
    "            action_ohe = action_ohe[None]\n",
    "        n =  torch.tensor([self.step_size/self.max_step]*self.batch_size)\n",
    "        n = n.unsqueeze(1).float().to(self.device) # (batch,1)\n",
    "        v = torch.cat([action_ohe,supp_curr,n], dim=1) # (batch,8+2+1)\n",
    "        with torch.no_grad():\n",
    "            s_next = self.model((s_curr,v)).detach()\n",
    "        reward = self.get_reward(s_next)\n",
    "        return s_next, supp_curr, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204725d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEMModel(nn.Module):\n",
    "    def __init__(self, threshold: int = 70):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 2)\n",
    "        self.linear1 = nn.Linear(7744, 256)\n",
    "        self.linear2 = nn.Linear(256, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.linear1(torch.flatten(x, 1)))\n",
    "        x = self.linear2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def train_step(self, states: List[List[torch.Tensor]], actions: List[List[int]], rewards: np.ndarray, batch_size=32, device=torch.device(\"cpu\")):\n",
    "        threshold = np.percentile(rewards, 70)\n",
    "        elite_session_indices = np.where(rewards > threshold)[0]\n",
    "        self.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=3e-5, weight_decay=1e-5)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "\n",
    "        for index in elite_session_indices:\n",
    "            for state in states[index]:\n",
    "                elite_states.append(state[None])\n",
    "            for action in actions[index]:\n",
    "                elite_actions.append(torch.LongTensor([action]))\n",
    "\n",
    "        dataset = TensorDataset(torch.cat(elite_states), torch.cat(elite_actions))\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "        for x, y_true in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self.forward(x.to(device))\n",
    "            loss = loss_fn(y_pred, y_true.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NeuralEnv(\"../best_models/resunet_v5.pth\",\n",
    "                \"../best_models/nostone_stone_classifier.pth\",\n",
    "                \"cpu\",\n",
    "                1)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ff497",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CEMModel().to(device)\n",
    "\n",
    "epoches = 10\n",
    "sessions_per_epoch = 10\n",
    "steps_per_session = 10\n",
    "for train_step in range(epoches):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = np.zeros(sessions_per_epoch)\n",
    "    for i in tqdm(range(sessions_per_epoch)):\n",
    "        session_states = []\n",
    "        session_actions = []\n",
    "\n",
    "        s_curr, supp_curr = env.reset()\n",
    "        for _ in range(steps_per_session):\n",
    "            probs = agent(s_curr)[0].detach().numpy()\n",
    "            chosen_action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            s_next, supp_next, reward = env.step(s_curr, supp_curr, torch.LongTensor([[chosen_action + 1]]).to(device))\n",
    "\n",
    "            session_states.append(s_next[0])\n",
    "            session_actions.append(chosen_action)\n",
    "            rewards[i] += reward.detach().numpy()\n",
    "\n",
    "            s_curr, supp_curr = s_next, supp_next\n",
    "        states.append(session_states)\n",
    "        actions.append(session_actions)\n",
    "    agent.train_step(states, actions, rewards)\n",
    "    print(f\"{train_step}: {rewards.mean():.2f}\")\n",
    "    torch.save(agent.state_dict(), \"../weights/agent.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e2606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
